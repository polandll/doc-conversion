# Failure detection and recovery {#archDataDistributeFailDetect .concept}

A method for locally determining from gossip state and history if another node in the system is down or has come back up.

Failure detection is a method for locally determining from gossip state and history if another node in the system is down or has come back up. Cassandra uses this information to avoid routing client requests to unreachable nodes whenever possible. \(Cassandra can also avoid routing requests to nodes that are alive, but performing poorly, through the [dynamic snitch](archSnitchesAbout.md).\)

The gossip process tracks state from other nodes both directly \(nodes gossiping directly to it\) and indirectly \(nodes communicated about secondhand, third-hand, and so on\). Rather than have a fixed threshold for marking failing nodes, Cassandra uses an accrual detection mechanism to calculate a per-node threshold that takes into account network performance, workload, and historical conditions. During gossip exchanges, every node maintains a sliding window of inter-arrival times of gossip messages from other nodes in the cluster. Configuring the [phi\_convict\_threshold](../configuration/configCassandra_yaml.md#phi_convict_threshold) property adjusts the sensitivity of the failure detector. Lower values increase the likelihood that an unresponsive node will be marked as down, while higher values decrease the likelihood that transient failures causing node failure. Use the default value for most situations, but increase it to 10 or 12 for Amazon EC2 \(due to frequently encountered network congestion\). In unstable network environments \(such as EC2 at times\), raising the value to 10 or 12 helps prevent false failures. Values higher than 12 and lower than 5 are not recommended.

Node failures can result from various causes such as hardware failures and network outages. Node outages are often transient but can last for extended periods. Because a node outage rarely signifies a permanent departure from the cluster it does not automatically result in permanent removal of the node from the ring. Other nodes will periodically try to re-establish contact with failed nodes to see if they are back up. To permanently change a node's membership in a cluster, administrators must explicitly add or remove nodes from a Cassandra cluster using the [nodetool utility](../tools/toolsNodetool.md).

When a node comes back online after an outage, it may have missed writes for the replica data it maintains. [Repair mechanisms](../operations/opsRepairNodesTOC.md) exist to recover missed data, such as hinted handoffs and manual repair with [nodetool repair](../tools/toolsRepair.md). The length of the outage will determine which repair mechanism is used to make the data consistent.

**Parent topic:** [Internode communications \(gossip\)](../../cassandra/architecture/archGossipAbout.md)

